# -*- coding: utf-8 -*-
"""RAG-Streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JVARGOUmKGXy1hX7sw8vV13xOCopvEky
"""

import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFaceHub

# --- Streamlit UI ---
st.title("ðŸ“‘ RAG Demo - Ask Questions about Your PDF")

# File uploader
uploaded_file = st.file_uploader("Upload a PDF", type=["pdf"])

if uploaded_file:
    with open("uploaded.pdf", "wb") as f:
        f.write(uploaded_file.read())

    st.info("Processing document...")

    # 1. Load PDF
    loader = PyPDFLoader("uploaded.pdf")
    documents = loader.load()

    # 2. Split text into chunks
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30)
    docs = text_splitter.split_documents(documents)

    # 3. Create embeddings
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # 4. Store in FAISS
    db = FAISS.from_documents(docs, embeddings)
    retriever = db.as_retriever(search_kwargs={"k": 2})

    # 5. Connect to HuggingFace LLM (requires API key)
    llm = HuggingFaceHub(
        repo_id="google/flan-t5-small",  # small model for demo
        huggingfacehub_api_token=st.secrets["HUGGINGFACEHUB_API_TOKEN"],
    )

    qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)

    # 6. Ask questions
    query = st.text_input("Ask a question about the document:")
    if query:
        with st.spinner("Thinking..."):
            answer = qa_chain.run(query)
        st.success(answer)

